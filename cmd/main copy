import (
	"fmt"
	"net/http"
	"net/url"
	"strings"
	"sync"
	"time"

	"golang.org/x/net/html"
)

// Crawler is a simple web crawler.
type Crawler struct {
	domain      string
	visitedURLs map[string]bool
	maxWorkers  int
	wg          sync.WaitGroup
	queue       chan *url.URL
	mutex       sync.Mutex
}

// NewCrawler creates a new crawler for a specific domain with a maximum number of workers.
func NewCrawler(domain string, maxWorkers int) *Crawler {
	return &Crawler{
		domain:      domain,
		visitedURLs: make(map[string]bool),
		maxWorkers:  maxWorkers,
		queue:       make(chan *url.URL, maxWorkers), // Buffered channel for controlling concurrency
	}
}

// IsSameDomain checks if a URL is from the same domain.
func (c *Crawler) IsSameDomain(u *url.URL) bool {
	if strings.Contains(u.Hostname(), c.domain) || strings.HasPrefix(u.String(), "/") {
		return true
	}
	return false
}

// VisitURL visits a URL, extracts links, and prints them.
func (c *Crawler) VisitURL(u *url.URL) {
	defer c.wg.Done()

	// if !c.IsSameDomain(u) {
	// 	return // Skip URLs from different domains
	// }

	c.mutex.Lock()
	if c.visitedURLs[u.String()] {
		c.mutex.Unlock()
		return // Skip already visited URLs
	}
	c.visitedURLs[u.String()] = true
	c.mutex.Unlock()

	fmt.Println("Visiting:", u)

	response, err := http.Get("https://www.altus.cr")
	if err != nil {
		fmt.Println("Error:", err)
		return
	}
	// defer response.Body.Close()

	tokenizer := html.NewTokenizer(response.Body)

	for {
		tokenType := tokenizer.Next()
		switch tokenType {
		case html.ErrorToken:
			return // Finished parsing the page
		case html.StartTagToken, html.SelfClosingTagToken:
			token := tokenizer.Token()
			if token.Data == "a" {
				for _, attr := range token.Attr {
					if attr.Key == "href" {
						linkURL, err := url.Parse(attr.Val)
						if err == nil && c.IsSameDomain(linkURL) {
							fmt.Println("  Found link:", linkURL)
							c.queue <- linkURL
						}
					}
				}
			}
		}
	}
}

func (c *Crawler) worker() {
	for {
		select {
		case url := <-c.queue:
			c.wg.Add(1)
			go c.VisitURL(url)
		}
	}
}

func main() {
	startURL := "altus.cr"
	maxWorkers := 5 // Set the maximum number of concurrent workers

	crawler := NewCrawler("altus.cr", maxWorkers)

	// Start the worker pool
	for i := 0; i < maxWorkers; i++ {
		go crawler.worker()
	}
	url, err := url.Parse(startURL)
	fmt.Println(err)
	crawler.queue <- url

	// Wait for all goroutines to finish
	crawler.wg.Wait()

	// Sleep for a moment to ensure all messages are printed before exiting
	time.Sleep(100 * time.Second)
}
